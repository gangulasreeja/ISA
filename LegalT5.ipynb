{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOScTnBkMup2hEY2bFr5xQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangulasreeja/ISA/blob/main/LegalT5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3i4ZICMj_J4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"ninadn/indian-legal\")"
      ],
      "metadata": {
        "id": "_421cF9FkIVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ElFDdihtkMYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input = 512\n",
        "max_target = 128\n",
        "batch_size = 3\n",
        "model_checkpoints = \"google/flan-t5-large\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoints)"
      ],
      "metadata": {
        "id": "MKSC1bAQkY0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data_to_process):\n",
        "    inputs = data_to_process['Text']\n",
        "    targets = data_to_process['Summary']\n",
        "    inputs = [\" \" if text is None else text for text in inputs]\n",
        "    targets = [\" \" if text is None else text for text in targets]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input, padding='max_length', truncation=True)\n",
        "    targets_tokenized = tokenizer(text_target=targets, max_length=max_target, padding='max_length', truncation=True)\n",
        "    model_inputs['labels'] = targets_tokenized['input_ids']\n",
        "    return model_inputs\n",
        "tokenize_data = ds.map(preprocess_data, batched=True)"
      ],
      "metadata": {
        "id": "ucTG7fAoks6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_valid = 200\n",
        "train_valid_split = tokenize_data['train'].train_test_split(test_size=num_valid, shuffle=True, seed=42)\n",
        "\n",
        "# Rename keys for clarity\n",
        "tokenize_data['train'] = train_valid_split['train']\n",
        "tokenize_data['valid'] = train_valid_split['test']\n",
        "\n",
        "# Print sizes to confirm\n",
        "print(f\"Train dataset size: {len(tokenize_data['train'])}\")\n",
        "print(f\"Validation dataset size: {len(tokenize_data['valid'])}\")\n",
        "print(f\"Text dataset size: {len(tokenize_data['test'])}\")"
      ],
      "metadata": {
        "id": "z2kglUVvlA5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_data"
      ],
      "metadata": {
        "id": "-fDeJi9nlF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
      ],
      "metadata": {
        "id": "9VvtIRbolJT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "import evaluate\n",
        "metric = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "nKlN0tTYlM0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge(pred):\n",
        "  predictions, labels = pred\n",
        "  #decode the predictions\n",
        "  decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  #decode labels\n",
        "  decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  #compute results\n",
        "  res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
        "  #get %\n",
        "  res = {key: value * 100 for key, value in res.items()}\n",
        "\n",
        "  pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  res['gen_len'] = np.mean(pred_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in res.items()}"
      ],
      "metadata": {
        "id": "7cocK-bilahe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = transformers.Seq2SeqTrainingArguments(\n",
        "    'legal-summ',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size= 1,\n",
        "    per_device_eval_batch_size= 1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    fp16=True\n",
        "    )"
      ],
      "metadata": {
        "id": "MoV_rBp-lbIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenize_data['train'],\n",
        "    eval_dataset=tokenize_data['valid'],\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_rouge\n",
        ")"
      ],
      "metadata": {
        "id": "0MdQkLgeld-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "S8xMgNWdlgPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-RhG8kvli7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}